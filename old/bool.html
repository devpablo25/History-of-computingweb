<html>
<head>
    <link rel="stylesheet" href="bool.css">
    <title>
        boolean
    </title>
</head>

<body>

    <header>
        <h1>boolean algebra</h1>
    </header>
<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSez0taOoEZTAerJR3ISRTOMI1tTegVdPTpmg&s" alt="">
<p>
    For the purposes of this definition it is irrelevant how the operations came to satisfy the laws, whether by fiat or
proof. All concrete Boolean algebras satisfy the laws (by proof rather than fiat), whence every concrete Boolean
algebra is a Boolean algebra according to our definitions. This axiomatic definition of a Boolean algebra as a set
and certain operations satisfying certain laws or axioms by fiat is entirely analogous to the abstract definitions of
group, ring, field etc. characteristic of modern or abstract algebra.
Given any complete axiomatization of Boolean algebra, such as the axioms for a complemented distributive
lattice, a sufficient condition for an algebraic structure of this kind to satisfy all the Boolean laws is that it
satisfy just those axioms. The following is therefore an equivalent definition.
A Boolean algebra is a complemented distributive lattice.
The section on axiomatization lists other axiomatizations, any of which can be made the basis of an equivalent
definition.
Although every concrete Boolean algebra is a Boolean algebra, not every Boolean algebra need be concrete. Let
n be a square-free positive integer, one not divisible by the square of an integer, for example 30 but not 12. The
operations of greatest common divisor, least common multiple, and division into n (that is, ¬x = n/x), can be
shown to satisfy all the Boolean laws when their arguments range over the positive divisors of n. Hence those
divisors form a Boolean algebra. These divisors are not subsets of a set, making the divisors of n a Boolean
algebra that is not concrete according to our definitions.
However, if each divisor of n is represented by the set of its prime factors, this nonconcrete Boolean algebra is
isomorphic to the concrete Boolean algebra consisting of all sets of prime factors of n, with union corresponding
to least common multiple, intersection to greatest common divisor, and complement to division into n. So this
example, while not technically concrete, is at least "morally" concrete via this representation, called an
isomorphism. This example is an instance of the following notion.
A Boolean algebra is called representable when it is isomorphic to a concrete Boolean algebra.
The next question is answered positively as follows.
Every Boolean algebra is representable.
That is, up to isomorphism, abstract and concrete Boolean algebras are the same thing. This result depends on
the Boolean prime ideal theorem, a choice principle slightly weaker than the axiom of choice. This strong
relationship implies a weaker result strengthening the observation in the previous subsection to the following
easy consequence of representability.
The laws satisfied by all Boolean algebras coincide with those satisfied by the prototypical
Boolean algebra.
It is weaker in the sense that it does not of itself imply representability. Boolean algebras are special here, for
example a relation algebra is a Boolean algebra with additional structure but it is not the case that every relation
algebra is representable in the sense appropriate to relation algebras.
Representable Boolean algebras
The above definition of an abstract Boolean algebra as a set together with operations satisfying "the" Boolean
laws raises the question of what those laws are. A simplistic answer is "all Boolean laws", which can be defined
as all equations that hold for the Boolean algebra of 0 and 1. However, since there are infinitely many such
laws, this is not a satisfactory answer in practice, leading to the question of it suffices to require only finitely
many laws to hold.
In the case of Boolean algebras, the answer is "yes": the finitely many equations listed above are sufficient.
Thus, Boolean algebra is said to be finitely axiomatizable or finitely based.
Moreover, the number of equations needed can be further reduced. To begin with, some of the above laws are
implied by some of the others. A sufficient subset of the above laws consists of the pairs of associativity,
commutativity, and absorption laws, distributivity of ∧ over ∨ (or the other distributivity law—one suffices),
and the two complement laws. In fact, this is the traditional axiomatization of Boolean algebra as a
complemented distributive lattice.
By introducing additional laws not listed above, it becomes possible to shorten the list of needed equations yet
further; for instance, with the vertical bar representing the Sheffer stroke operation, the single axiom
is sufficient to completely axiomatize Boolean algebra. It is also possible
to find longer single axioms using more conventional operations; see Minimal axioms for Boolean algebra.[30]
Propositional logic is a logical system that is intimately connected to Boolean algebra.[5] Many syntactic
concepts of Boolean algebra carry over to propositional logic with only minor changes in notation and
terminology, while the semantics of propositional logic are defined via Boolean algebras in a way that the
tautologies (theorems) of propositional logic correspond to equational theorems of Boolean algebra.
Syntactically, every Boolean term corresponds to a propositional formula of propositional logic. In this
translation between Boolean algebra and propositional logic, Boolean variables x, y, ... become propositional
variables (or atoms) P, Q, ... Boolean terms such as x ∨ y become propositional formulas P ∨ Q; 0 becomes
false or ⊥, and 1 becomes true or T. It is convenient when referring to generic propositions to use Greek letters
Φ, Ψ, ... as metavariables (variables outside the language of propositional calculus, used when talking about
propositional calculus) to denote propositions.
The semantics of propositional logic rely on truth assignments. The essential idea of a truth assignment is that
the propositional variables are mapped to elements of a fixed Boolean algebra, and then the truth value of a
propositional formula using these letters is the element of the Boolean algebra that is obtained by computing the
value of the Boolean term corresponding to the formula. In classical semantics, only the two-element Boolean
algebra is used, while in Boolean-valued semantics arbitrary Boolean algebras are considered. A tautology is a
propositional formula that is assigned truth value 1 by every truth assignment of its propositional variables to an
arbitrary Boolean algebra (or, equivalently, every truth assignment to the two element Boolean algebra).
These semantics permit a translation between tautologies of propositional logic and equational theorems of
Boolean algebra. Every tautology Φ of propositional logic can be expressed as the Boolean equation Φ = 1,
which will be a theorem of Boolean algebra. Conversely, every theorem Φ = Ψ of Boolean algebra corresponds
Axiomatizing Boolean algebra
Propositional logic
to the tautologies (Φ ∨ ¬Ψ) ∧ (¬Φ ∨ Ψ) and (Φ ∧ Ψ) ∨ (¬Φ ∧ ¬Ψ). If → is in the language, these last
tautologies can also be written as (Φ → Ψ) ∧ (Ψ → Φ), or as two separate theorems Φ → Ψ and Ψ → Φ; if ≡ is
available, then the single tautology Φ ≡ Ψ can be used.
One motivating application of propositional calculus is the analysis of propositions and deductive arguments in
natural language.[31] Whereas the proposition "if x = 3, then x + 1 = 4" depends on the meanings of such
symbols as + and 1, the proposition "if x = 3, then x = 3" does not; it is true merely by virtue of its structure, and
remains true whether "x = 3" is replaced by "x = 4" or "the moon is made of green cheese." The generic or
abstract form of this tautology is "if P, then P," or in the language of Boolean algebra, P → P.
Replacing P by x = 3 or any other proposition is called instantiation of P by that proposition. The result of
instantiating P in an abstract proposition is called an instance of the proposition. Thus, x = 3 → x = 3 is a
tautology by virtue of being an instance of the abstract tautology P → P. All occurrences of the instantiated
variable must be instantiated with the same proposition, to avoid such nonsense as P → x = 3 or x = 3 → x = 4.
Propositional calculus restricts attention to abstract propositions, those built up from propositional variables
using Boolean operations. Instantiation is still possible within propositional calculus, but only by instantiating
propositional variables by abstract propositions, such as instantiating Q by Q → P in P → (Q → P) to yield the
instance P → ((Q → P) → P).
(The availability of instantiation as part of the machinery of propositional calculus avoids the need for
metavariables within the language of propositional calculus, since ordinary propositional variables can be
considered within the language to denote arbitrary propositions. The metavariables themselves are outside the
reach of instantiation, not being part of the language of propositional calculus but rather part of the same
language for talking about it that this sentence is written in, where there is a need to be able to distinguish
propositional variables and their instantiations as being distinct syntactic entities.)
An axiomatization of propositional calculus is a set of tautologies called axioms and one or more inference rules
for producing new tautologies from old. A proof in an axiom system A is a finite nonempty sequence of
propositions each of which is either an instance of an axiom of A or follows by some rule of A from propositions
appearing earlier in the proof (thereby disallowing circular reasoning). The last proposition is the theorem
proved by the proof. Every nonempty initial segment of a proof is itself a proof, whence every proposition in a
proof is itself a theorem. An axiomatization is sound when every theorem is a tautology, and complete when
every tautology is a theorem.[32]
Propositional calculus is commonly organized as a Hilbert system, whose operations are just those of Boolean
algebra and whose theorems are Boolean tautologies, those Boolean terms equal to the Boolean constant 1.
Another form is sequent calculus, which has two sorts, propositions as in ordinary propositional calculus, and
pairs of lists of propositions called sequents, such as A ∨ B, A ∧ C, ... ⊢ A, B → C, .... The two halves of
a sequent are called the antecedent and the succedent respectively. The customary metavariable denoting an
antecedent or part thereof is Γ, and for a succedent Δ; thus Γ, A ⊢ Δ would denote a sequent whose succedent is
a list Δ and whose antecedent is a list Γ with an additional proposition A appended after it. The antecedent is
interpreted as the conjunction of its propositions, the succedent as the disjunction of its propositions, and the
sequent itself as the entailment of the succedent by the antecedent.
Applications
Deductive systems for propositional logic
Sequent calculus
Entailment differs from implication in that whereas the latter is a binary operation that returns a value in a
Boolean algebra, the former is a binary relation which either holds or does not hold. In this sense, entailment is
an external form of implication, meaning external to the Boolean algebra, thinking of the reader of the sequent
as also being external and interpreting and comparing antecedents and succedents in some Boolean algebra. The
natural interpretation of ⊢ is as ≤ in the partial order of the Boolean algebra defined by x ≤ y just when
x ∨ y = y. This ability to mix external implication ⊢ and internal implication → in the one logic is among the
essential differences between sequent calculus and propositional calculus.[33]
Boolean algebra as the calculus of two values is fundamental to computer circuits, computer programming, and
mathematical logic, and is also used in other areas of mathematics such as set theory and statistics.[5]
In the early 20th century, several electrical engineers intuitively recognized that Boolean algebra was analogous
to the behavior of certain types of electrical circuits. Claude Shannon formally proved such behavior was
logically equivalent to Boolean algebra in his 1937 master's thesis, A Symbolic Analysis of Relay and Switching
Circuits.
Today, all modern general-purpose computers perform their functions using two-value Boolean logic; that is,
their electrical circuits are a physical manifestation of two-value Boolean logic. They achieve this in various
ways: as voltages on wires in high-speed circuits and capacitive storage devices, as orientations of a magnetic
domain in ferromagnetic storage devices, as holes in punched cards or paper tape, and so on. (Some early
computers used decimal circuits or mechanisms instead of two-valued logic circuits.)
Of course, it is possible to code more than two symbols in any given medium. For example, one might use
respectively 0, 1, 2, and 3 volts to code a four-symbol alphabet on a wire, or holes of different sizes in a
punched card. In practice, the tight constraints of high speed, small size, and low power combine to make noise
a major factor. This makes it hard to distinguish between symbols when there are several possible symbols that
could occur at a single site. Rather than attempting to distinguish between four voltages on one wire, digital
designers have settled on two voltages per wire, high and low.
Computers use two-value Boolean circuits for the above reasons. The most common computer architectures use
ordered sequences of Boolean values, called bits, of 32 or 64 values, e.g.
01101000110101100101010101001011. When programming in machine code, assembly language, and certain
other programming languages, programmers work with the low-level digital structure of the data registers.
These registers operate on voltages, where zero volts represents Boolean 0, and a reference voltage (often +5 V,
+3.3 V, or +1.8 V) represents Boolean 1. Such languages support both numeric operations and logical
operations. In this context, "numeric" means that the computer treats sequences of bits as binary numbers (base
two numbers) and executes arithmetic operations like add, subtract, multiply, or divide. "Logical" refers to the
Boolean logical operations of disjunction, conjunction, and negation between two sequences of bits, in which
each bit in one sequence is simply compared to its counterpart in the other sequence. Programmers therefore
have the option of working in and applying the rules of either numeric algebra or Boolean algebra as needed. A
core differentiating feature between these families of operations is the existence of the carry operation in the
first but not the second.
Applications
Computers
Other areas where two values is a good choice are the law and mathematics. In everyday relaxed conversation,
nuanced or complex answers such as "maybe" or "only on the weekend" are acceptable. In more focused
situations such as a court of law or theorem-based mathematics, however, it is deemed advantageous to frame
questions so as to admit a simple yes-or-no answer—is the defendant guilty or not guilty, is the proposition true
or false—and to disallow any other answer. However, limiting this might prove in practice for the respondent,
the principle of the simple yes–no question has become a central feature of both judicial and mathematical logic,
making two-valued logic deserving of organization and study in its own right.
A central concept of set theory is membership. An organization may permit multiple degrees of membership,
such as novice, associate, and full. With sets, however, an element is either in or out. The candidates for
membership in a set work just like the wires in a digital computer: each candidate is either a member or a
nonmember, just as each wire is either high or low.
Algebra being a fundamental tool in any area amenable to mathematical treatment, these considerations
combine to make the algebra of two values of fundamental importance to computer hardware, mathematical
logic, and set theory.
Two-valued logic can be extended to multi-valued logic, notably by replacing the Boolean domain {0, 1} with
the unit interval [0,1], in which case rather than only taking values 0 or 1, any value between and including 0
and 1 can be assumed. Algebraically, negation (NOT) is replaced with 1 − x, conjunction (AND) is replaced
with multiplication (xy), and disjunction (OR) is defined via De Morgan's law. Interpreting these values as
logical truth values yields a multi-valued logic, which forms the basis for fuzzy logic and probabilistic logic. In
these interpretations, a value is interpreted as the "degree" of truth – to what extent a proposition is true, or the
probability that the proposition is true.
The original application for Boolean operations was mathematical logic, where it combines the truth values, true
or false, of individual formulas.
Natural languages such as English have words for several Boolean operations, in particular conjunction (and),
disjunction (or), negation (not), and implication (implies). But not is synonymous with and not. When used to
combine situational assertions such as "the block is on the table" and "cats drink milk", which naïvely are either
true or false, the meanings of these logical connectives often have the meaning of their logical counterparts.
However, with descriptions of behavior such as "Jim walked through the door", one starts to notice differences
such as failure of commutativity, for example, the conjunction of "Jim opened the door" with "Jim walked
through the door" in that order is not equivalent to their conjunction in the other order, since and usually means
and then in such cases. Questions can be similar: the order "Is the sky blue, and why is the sky blue?" makes
more sense than the reverse order. Conjunctive commands about behavior are like behavioral assertions, as in
get dressed and go to school. Disjunctive commands such love me or leave me or fish or cut bait tend to be
asymmetric via the implication that one alternative is less preferable. Conjoined nouns such as tea and milk
generally describe aggregation as with set union while tea or milk is a choice. However, context can reverse
these senses, as in your choices are coffee and tea which usually means the same as your choices are coffee or
tea (alternatives). Double negation, as in "I don't not like milk", rarely means literally "I do like milk" but rather
conveys some sort of hedging, as though to imply that there is a third possibility. "Not not P" can be loosely
Two-valued logic
Boolean operations
Natural language
interpreted as "surely P", and although P necessarily implies "not not P," the converse is suspect in English,
much as with intuitionistic logic. In view of the highly idiosyncratic usage of conjunctions in natural languages,
Boolean algebra cannot be considered a reliable framework for interpreting them.
Boolean operations are used in digital logic to combine the bits carried on individual wires, thereby interpreting
them over {0,1}. When a vector of n identical binary gates are used to combine two bit vectors each of n bits,
the individual bit operations can be understood collectively as a single operation on values from a Boolean
algebra with 2n elements.
Naive set theory interprets Boolean operations as acting on subsets of a given set X. As we saw earlier this
behavior exactly parallels the coordinate-wise combinations of bit vectors, with the union of two sets
corresponding to the disjunction of two bit vectors and so on.
The 256-element free Boolean algebra on three generators is deployed in computer displays based on raster
graphics, which use bit blit to manipulate whole regions consisting of pixels, relying on Boolean operations to
specify how the source region should be combined with the destination, typically with the help of a third region
called the mask. Modern video cards offer all 223
= 256 ternary operations for this purpose, with the choice of
operation being a one-byte (8-bit) parameter. The constants SRC = 0xaa or 0b10101010, DST = 0xcc or
0b11001100, and MSK = 0xf0 or 0b11110000 allow Boolean operations such as (SRC^DST)&MSK
(meaning XOR the source and destination and then AND the result with the mask) to be written directly as a
constant denoting a byte calculated at compile time, 0x80 in the (SRC^DST)&MSK example, 0x88 if just
SRC^DST, etc. At run time the video card interprets the byte as the raster operation indicated by the original
expression in a uniform way that requires remarkably little hardware and which takes time completely
independent of the complexity of the expression.
Solid modeling systems for computer aided design offer a variety of methods for building objects from other
objects, combination by Boolean operations being one of them. In this method the space in which objects exist
is understood as a set S of voxels (the three-dimensional analogue of pixels in two-dimensional graphics) and
shapes are defined as subsets of S, allowing objects to be combined as sets via union, intersection, etc. One
obvious use is in building a complex shape from simple shapes simply as the union of the latter. Another use is
in sculpting understood as removal of material: any grinding, milling, routing, or drilling operation that can be
performed with physical machinery on physical materials can be simulated on the computer with the Boolean
operation x ∧ ¬y or x − y, which in set theory is set difference, remove the elements of y from those of x. Thus
given two shapes one to be machined and the other the material to be removed, the result of machining the
former to remove the latter is described simply as their set difference

Sorce: wikipeada
</p>

</body>

</html>
